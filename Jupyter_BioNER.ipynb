{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ecc79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import datetime\n",
    "import time\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import codecs\n",
    "from model.crf import *\n",
    "from model.lm_lstm_crf import *\n",
    "import model.utils as utils\n",
    "from model.evaluator import eval_wc\n",
    "from model.predictor import predict_wc #NEW\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import functools\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eff4a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1c92e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    train_file = [\"data_bioner_5/BC2GM-IOBES/merge.tsv\",\n",
    "                  \"data_bioner_5/BC4CHEMD-IOBES/merge.tsv\",\n",
    "                  \"data_bioner_5/BC5CDR-IOBES/merge.tsv\",\n",
    "                  \"data_bioner_5/NCBI-disease-IOBES/merge.tsv\",\n",
    "                  \"data_bioner_5/JNLPBA-IOBES/merge.tsv\"]\n",
    "    \n",
    "    dev_file = [\"data_bioner_5/BC2GM-IOBES/devel.tsv\",\n",
    "                  \"data_bioner_5/BC4CHEMD-IOBES/devel.tsv\",\n",
    "                  \"data_bioner_5/BC5CDR-IOBES/devel.tsv\",\n",
    "                  \"data_bioner_5/NCBI-disease-IOBES/devel.tsv\",\n",
    "                  \"data_bioner_5/JNLPBA-IOBES/devel.tsv\"]\n",
    "    \n",
    "    test_file = [\"data_bioner_5/BC2GM-IOBES/test.tsv\",\n",
    "                  \"data_bioner_5/BC4CHEMD-IOBES/test.tsv\",\n",
    "                  \"data_bioner_5/BC5CDR-IOBES/test.tsv\",\n",
    "                  \"data_bioner_5/NCBI-disease-IOBES/test.tsv\",\n",
    "                  \"data_bioner_5/JNLPBA-IOBES/test.tsv\"]\n",
    "    \n",
    "    rand_embedding = False\n",
    "    emb_file = 'data_bioner_5/wikipedia-pubmed-and-PMC-w2v.txt'\n",
    "    gpu = 1\n",
    "    batch_size = 80\n",
    "    unk = 'unk'\n",
    "    char_hidden = 300\n",
    "    word_hidden = 300\n",
    "    drop_out = 0.5\n",
    "    epoch = 100\n",
    "    start_epoch = 0\n",
    "    checkpoint = './checkpoint/model_{}'.format(epoch)\n",
    "    caseless = True\n",
    "    char_dim = 30\n",
    "    word_dim = 200\n",
    "    char_layers = 1\n",
    "    word_layers = 1\n",
    "    lr = 0.01\n",
    "    lr_decay = 0.05\n",
    "    fine_tune = False\n",
    "    load_check_point = ''\n",
    "    load_opt = False\n",
    "    update = 'adam'\n",
    "    momentum = 0.9\n",
    "    clip_grad = 5.0\n",
    "    small_crf = True\n",
    "    mini_count = 5\n",
    "    lambda0 = 1\n",
    "    co_train = False\n",
    "    patience = 30\n",
    "    high_way = False\n",
    "    highway_layers = 1\n",
    "    eva_matrix = 'fa'\n",
    "    least_iters = 50\n",
    "    shrink_embedding = True\n",
    "    output_annotation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05cbfa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    }
   ],
   "source": [
    "print(args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "523eb66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting:\n",
      "<class '__main__.args'>\n"
     ]
    }
   ],
   "source": [
    "if args.gpu >= 0:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "print('setting:')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4765a5",
   "metadata": {},
   "source": [
    "- loading corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a377f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus\n"
     ]
    }
   ],
   "source": [
    "# load corpus\n",
    "print('loading corpus')\n",
    "file_num = len(args.train_file)\n",
    "lines = []\n",
    "dev_lines = []\n",
    "test_lines = []\n",
    "for i in range(file_num):\n",
    "    with codecs.open(args.train_file[i], 'r', 'utf-8') as f:\n",
    "        lines0 = f.readlines()\n",
    "    lines.append(lines0)\n",
    "for i in range(file_num):\n",
    "    with codecs.open(args.dev_file[i], 'r', 'utf-8') as f:\n",
    "        dev_lines0 = f.readlines()\n",
    "    dev_lines.append(dev_lines0)\n",
    "for i in range(file_num):\n",
    "    with codecs.open(args.test_file[i], 'r', 'utf-8') as f:\n",
    "        test_lines0 = f.readlines()\n",
    "    test_lines.append(test_lines0)\n",
    "\n",
    "dataset_loader = []\n",
    "dev_dataset_loader = []\n",
    "test_dataset_loader = []\n",
    "f_map = dict()\n",
    "l_map = dict()\n",
    "char_count = dict()\n",
    "train_features = []\n",
    "dev_features = []\n",
    "test_features = []\n",
    "train_labels = []\n",
    "dev_labels = []\n",
    "test_labels = []\n",
    "train_features_tot = []\n",
    "test_word = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5c522e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing coding table\n",
      "constructing coding table\n",
      "constructing coding table\n",
      "constructing coding table\n",
      "constructing coding table\n"
     ]
    }
   ],
   "source": [
    "# load corpus con't\n",
    "for i in range(file_num):\n",
    "    dev_features0, dev_labels0 = utils.read_corpus(dev_lines[i])\n",
    "    test_features0, test_labels0 = utils.read_corpus(test_lines[i])\n",
    "\n",
    "    dev_features.append(dev_features0)\n",
    "    test_features.append(test_features0)\n",
    "    dev_labels.append(dev_labels0)\n",
    "    test_labels.append(test_labels0)\n",
    "\n",
    "    if args.output_annotation: #NEW\n",
    "        test_word0 = utils.read_features(test_lines[i])\n",
    "        test_word.append(test_word0)\n",
    "\n",
    "    if args.load_check_point:\n",
    "        if os.path.isfile(args.load_check_point):\n",
    "            print(\"loading checkpoint: '{}'\".format(args.load_check_point))\n",
    "            checkpoint_file = torch.load(args.load_check_point)\n",
    "            args.start_epoch = checkpoint_file['epoch']\n",
    "            f_map = checkpoint_file['f_map']\n",
    "            l_map = checkpoint_file['l_map']\n",
    "            c_map = checkpoint_file['c_map']\n",
    "            in_doc_words = checkpoint_file['in_doc_words']\n",
    "            train_features, train_labels = utils.read_corpus(lines[i])\n",
    "        else:\n",
    "            print(\"no checkpoint found at: '{}'\".format(args.load_check_point))\n",
    "    else:\n",
    "        print('constructing coding table')\n",
    "        train_features0, train_labels0, f_map, l_map, char_count = \\\n",
    "        utils.generate_corpus_char(lines[i], f_map, l_map, \n",
    "                                   char_count, c_thresholds=args.mini_count, if_shrink_w_feature=False)\n",
    "\n",
    "    train_features.append(train_features0)\n",
    "    train_labels.append(train_labels0)\n",
    "\n",
    "    train_features_tot += train_features0\n",
    "\n",
    "shrink_char_count = [k for (k, v) in iter(char_count.items()) if v >= args.mini_count]\n",
    "char_map = {shrink_char_count[ind]: ind for ind in range(0, len(shrink_char_count))}\n",
    "\n",
    "char_map['<u>'] = len(char_map)  # unk for char\n",
    "char_map[' '] = len(char_map)  # concat for char\n",
    "char_map['\\n'] = len(char_map)  # eof for char\n",
    "\n",
    "f_set = {v for v in f_map}\n",
    "dt_f_set = f_set\n",
    "f_map = utils.shrink_features(f_map, train_features_tot, args.mini_count)\n",
    "\n",
    "l_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5664cb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "data_bioner_5/wikipedia-pubmed-and-PMC-w2v.txt\n",
      "f_map\n",
      "107553\n",
      "True\n",
      "unk\n",
      "200\n",
      "True\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(args.fine_tune)\n",
    "print(args.emb_file)\n",
    "print(\"f_map\")\n",
    "print(len(dt_f_set))\n",
    "print(args.caseless)\n",
    "print(args.unk)\n",
    "print(args.word_dim)\n",
    "print(args.shrink_embedding)\n",
    "print(args.patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b50e9893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size: '26220'\n",
      "loading embedding\n",
      "embedding size: '68437'\n",
      "constructing dataset\n"
     ]
    }
   ],
   "source": [
    "for i in range(file_num):\n",
    "\n",
    "    dt_f_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), dev_features[i]), dt_f_set)\n",
    "    dt_f_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), test_features[i]), dt_f_set)\n",
    "\n",
    "    l_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), dev_labels[i]), l_set)\n",
    "    l_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), test_labels[i]), l_set)\n",
    "\n",
    "if not args.rand_embedding:\n",
    "    print(\"feature size: '{}'\".format(len(f_map)))\n",
    "    print('loading embedding')\n",
    "    if args.fine_tune:  # which means does not do fine-tune\n",
    "        f_map = {'<eof>': 0}\n",
    "    f_map, embedding_tensor, in_doc_words = utils.load_embedding_wlm(args.emb_file, ' ', f_map, dt_f_set, args.caseless, args.unk, args.word_dim, shrink_to_corpus=args.shrink_embedding)\n",
    "    print(\"embedding size: '{}'\".format(len(f_map)))\n",
    "\n",
    "for label in l_set:\n",
    "    if label not in l_map:\n",
    "        l_map[label] = len(l_map)\n",
    "\n",
    "print('constructing dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12f5cf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File:0\n",
      "\n",
      "After train 0 in 159.03229427337646s\n",
      "\n",
      "After dev 0 in 4.502795457839966s\n",
      "\n",
      "After test 0 in 17.93623995780945s\n",
      "\n",
      "After train_loader 0 in 0.0003192424774169922s\n",
      "\n",
      "After dev_loader 0 in 7.295608520507812e-05s\n",
      "\n",
      "After test_loader 0 in 7.05718994140625e-05s\n",
      "\n",
      "File:1\n",
      "\n",
      "After train 1 in 4312.685806274414s\n",
      "\n",
      "After dev 1 in 685.8849020004272s\n",
      "\n",
      "After test 1 in 512.8247437477112s\n",
      "\n",
      "After train_loader 1 in 0.0001361370086669922s\n",
      "\n",
      "After dev_loader 1 in 6.818771362304688e-05s\n",
      "\n",
      "After test_loader 1 in 6.651878356933594e-05s\n",
      "\n",
      "File:2\n",
      "\n",
      "After train 2 in 55.187166690826416s\n",
      "\n",
      "After dev 2 in 13.343783855438232s\n",
      "\n",
      "After test 2 in 15.141234397888184s\n",
      "\n",
      "After train_loader 2 in 0.00015115737915039062s\n",
      "\n",
      "After dev_loader 2 in 6.890296936035156e-05s\n",
      "\n",
      "After test_loader 2 in 6.604194641113281e-05s\n",
      "\n",
      "File:3\n",
      "\n",
      "After train 3 in 25.866610765457153s\n",
      "\n",
      "After dev 3 in 0.6279408931732178s\n",
      "\n",
      "After test 3 in 0.660031795501709s\n",
      "\n",
      "After train_loader 3 in 0.0001804828643798828s\n",
      "\n",
      "After dev_loader 3 in 6.532669067382812e-05s\n",
      "\n",
      "After test_loader 3 in 6.341934204101562e-05s\n",
      "\n",
      "File:4\n",
      "\n",
      "After train 4 in 276.9744670391083s\n",
      "\n",
      "After dev 4 in 2.552541971206665s\n",
      "\n",
      "After test 4 in 11.515860557556152s\n",
      "\n",
      "After train_loader 4 in 0.0001304149627685547s\n",
      "\n",
      "After dev_loader 4 in 8.130073547363281e-05s\n",
      "\n",
      "After test_loader 4 in 6.532669067382812e-05s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(file_num):\n",
    "    # construct dataset\n",
    "    print('File:{}\\n'.format(i))\n",
    "    tmp_st = time.time()\n",
    "    dataset, forw_corp, back_corp = utils.construct_bucket_mean_vb_wc(train_features[i], train_labels[i], l_map, char_map, f_map, args.caseless)\n",
    "    tmp_en = time.time()\n",
    "    print('After train {} in {}s\\n'.format(i, tmp_en-tmp_st))\n",
    "    tmp_st = time.time()\n",
    "    dev_dataset, forw_dev, back_dev = utils.construct_bucket_mean_vb_wc(dev_features[i], dev_labels[i], l_map, char_map, f_map, args.caseless)\n",
    "    tmp_en = time.time()\n",
    "    print('After dev {} in {}s\\n'.format(i, tmp_en-tmp_st))\n",
    "    tmp_st = time.time()\n",
    "    test_dataset, forw_test, back_test = utils.construct_bucket_mean_vb_wc(test_features[i], test_labels[i], l_map, char_map, f_map, args.caseless)\n",
    "    tmp_en = time.time()\n",
    "    print('After test {} in {}s\\n'.format(i, tmp_en-tmp_st))\n",
    "\n",
    "    tmp_st = time.time()\n",
    "    dataset_loader.append([torch.utils.data.DataLoader(tup, args.batch_size, shuffle=True, drop_last=False) for tup in dataset])\n",
    "    tmp_en = time.time()\n",
    "    print('After train_loader {} in {}s\\n'.format(i, tmp_en - tmp_st))\n",
    "    tmp_st = time.time()\n",
    "    dev_dataset_loader.append([torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in dev_dataset])\n",
    "    tmp_en = time.time()\n",
    "    print('After dev_loader {} in {}s\\n'.format(i, tmp_en-tmp_st))\n",
    "    tmp_st = time.time()\n",
    "    test_dataset_loader.append([torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in test_dataset])\n",
    "    tmp_en = time.time()\n",
    "    print('After test_loader {} in {}s\\n'.format(i, tmp_en-tmp_st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8b81b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tingyang/.conda/envs/adv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/home/tingyang/Github/Multi-BioNER/model/utils.py:797: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(weight, -bias, bias)\n",
      "/home/tingyang/Github/Multi-BioNER/model/utils.py:800: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(weight, -bias, bias)\n",
      "/home/tingyang/Github/Multi-BioNER/model/utils.py:786: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(input_linear.weight, -bias, bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 1\n"
     ]
    }
   ],
   "source": [
    "print('building model')\n",
    "ner_model = LM_LSTM_CRF(len(l_map), len(char_map), args.char_dim, args.char_hidden, args.char_layers, args.word_dim, args.word_hidden, args.word_layers, len(f_map), args.drop_out, file_num, large_CRF=args.small_crf, if_highway=args.high_way, in_doc_words=in_doc_words, highway_layers = args.highway_layers)\n",
    "\n",
    "if args.load_check_point:\n",
    "    ner_model.load_state_dict(checkpoint_file['state_dict'])\n",
    "else:\n",
    "    if not args.rand_embedding:\n",
    "        ner_model.load_pretrained_word_embedding(embedding_tensor)\n",
    "    ner_model.rand_init(init_word_embedding=args.rand_embedding)\n",
    "\n",
    "if args.update == 'sgd':\n",
    "    optimizer = optim.SGD(ner_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "elif args.update == 'adam':\n",
    "    optimizer = optim.Adam(ner_model.parameters(), lr=args.lr)\n",
    "\n",
    "if args.load_check_point and args.load_opt:\n",
    "    optimizer.load_state_dict(checkpoint_file['optimizer'])\n",
    "\n",
    "crit_lm = nn.CrossEntropyLoss()\n",
    "crit_ner = CRFLoss_vb(len(l_map), l_map['<start>'], l_map['<pad>'])\n",
    "\n",
    "if args.gpu >= 0:\n",
    "    if_cuda = True\n",
    "    print('device: ' + str(args.gpu))\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    crit_ner.cuda()\n",
    "    crit_lm.cuda()\n",
    "    ner_model.cuda()\n",
    "    packer = CRFRepack_WC(len(l_map), True)\n",
    "else:\n",
    "    if_cuda = False\n",
    "    packer = CRFRepack_WC(len(l_map), False)\n",
    "\n",
    "tot_length = sum(map(lambda t: len(t), dataset_loader))\n",
    "\n",
    "best_f1 = []\n",
    "for i in range(file_num):\n",
    "    best_f1.append(float('-inf'))\n",
    "\n",
    "best_pre = []\n",
    "for i in range(file_num):\n",
    "    best_pre.append(float('-inf'))\n",
    "\n",
    "best_rec = []\n",
    "for i in range(file_num):\n",
    "    best_rec.append(float('-inf'))\n",
    "\n",
    "track_list = list()\n",
    "start_time = time.time()\n",
    "epoch_list = range(args.start_epoch, args.start_epoch + args.epoch)\n",
    "patience_count = 0\n",
    "\n",
    "evaluator = eval_wc(packer, l_map, args.eva_matrix)\n",
    "\n",
    "predictor = predict_wc(if_cuda, f_map, char_map, l_map, f_map['<eof>'], char_map['\\n'], l_map['<pad>'], l_map['<start>'], True, args.batch_size, args.caseless) #NEW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83c05c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<model.crf.CRFRepack_WC object at 0x7f8ebb6cd360>\n",
      "{'O': 0, 'B-GENE': 1, 'I-GENE': 2, 'E-GENE': 3, 'S-GENE': 4, '<start>': 5, '<pad>': 6, 'S-Chemical': 7, 'B-Chemical': 8, 'E-Chemical': 9, 'I-Chemical': 10, 'B-Disease': 11, 'E-Disease': 12, 'I-Disease': 13, 'S-Disease': 14, 'B-DNA': 15, 'E-DNA': 16, 'B-protein': 17, 'E-protein': 18, 'S-protein': 19, 'I-protein': 20, 'B-cell_type': 21, 'I-cell_type': 22, 'E-cell_type': 23, 'I-DNA': 24, 'S-cell_type': 25, 'S-DNA': 26, 'S-cell_line': 27, 'B-cell_line': 28, 'E-cell_line': 29, 'I-cell_line': 30, 'B-RNA': 31, 'I-RNA': 32, 'E-RNA': 33, 'S-RNA': 34}\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(ner_model.parameters(), lr=args.lr)\n",
    "print(packer)\n",
    "print(l_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6b153ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 5)\n",
      "fa\n",
      "yeah\n",
      "1\n",
      "5\n",
      "range(0, 100)\n"
     ]
    }
   ],
   "source": [
    "print(epoch_list)\n",
    "print(args.eva_matrix)\n",
    "if 'f' in args.eva_matrix:\n",
    "    print(\"yeah\")\n",
    "    \n",
    "print(file_no)\n",
    "print(len(dataset_loader))\n",
    "mmp = map(lambda t: len(t), dataset_loader)\n",
    "epoch_list = range(args.start_epoch, args.start_epoch + args.epoch)\n",
    "print(epoch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85357a18",
   "metadata": {},
   "source": [
    "## Start Training in epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a23cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " - Tot it 20 (epoch 0):   0%|                                                                           | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14907/2792602752.py:43: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(ner_model.parameters(), args.clip_grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(loss: 5.0275, epoch: 0, dataset: 3, dev F1 = 0.8773, dev pre = 0.8870, dev rec = 0.8679, F1 on test = 0.7945, pre on test= 0.8485, rec on test= 0.7469), saving...\n",
      "epoch: 0\t in 100 take: 6.446596622467041 s\n",
      "(loss: 9.1468, epoch: 1, dataset: 2, dev F1 = 0.9388, dev pre = 0.9464, dev rec = 0.9314)                                     \n",
      "epoch: 1\t in 100 take: 16.524696588516235 s\n",
      "(loss: 17.0435, epoch: 2, dataset: 0, dev F1 = 0.8089, dev pre = 0.8109, dev rec = 0.8069, F1 on test = 0.7183, pre on test= 0.7337, rec on test= 0.7036), saving...\n",
      "epoch: 2\t in 100 take: 35.71427869796753 s\n",
      "                                                                                                                              \r"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(model.crf)\n",
    "importlib.reload(model.evaluator)\n",
    "from model.evaluator import eval_wc\n",
    "#importlib.reload(eval_wc)\n",
    "evaluator = eval_wc(packer, l_map, args.eva_matrix)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch_idx, args.start_epoch in enumerate(epoch_list):\n",
    "\n",
    "    sample_num = 1\n",
    "\n",
    "    epoch_loss = 0\n",
    "    ner_model.train()\n",
    "\n",
    "    for sample_id in tqdm( range(sample_num) , mininterval=2,\n",
    "            desc=' - Tot it %d (epoch %d)' % (tot_length, args.start_epoch), leave=False, file=sys.stdout):\n",
    "        \n",
    "        file_no = random.randint(0, file_num-1)            \n",
    "        cur_dataset = dataset_loader[file_no]\n",
    "\n",
    "        for f_f, f_p, b_f, b_p, w_f, tg_v, mask_v, len_v in itertools.chain.from_iterable(cur_dataset):\n",
    "\n",
    "            f_f, f_p, b_f, b_p, w_f, tg_v, mask_v = packer.repack_vb(f_f, f_p, b_f, b_p, w_f, tg_v, mask_v, len_v)\n",
    "\n",
    "            ner_model.zero_grad()\n",
    "            scores = ner_model(f_f, f_p, b_f, b_p, w_f, file_no)\n",
    "            loss = crit_ner(scores, tg_v, mask_v)\n",
    "\n",
    "            epoch_loss += utils.to_scalar(loss)\n",
    "            if args.co_train:\n",
    "                cf_p = f_p[0:-1, :].contiguous()\n",
    "                cb_p = b_p[1:, :].contiguous()\n",
    "                cf_y = w_f[1:, :].contiguous()\n",
    "                cb_y = w_f[0:-1, :].contiguous()\n",
    "                cfs, _ = ner_model.word_pre_train_forward(f_f, cf_p)\n",
    "                loss = loss + args.lambda0 * crit_lm(cfs, cf_y.view(-1))\n",
    "                cbs, _ = ner_model.word_pre_train_backward(b_f, cb_p)\n",
    "                loss = loss + args.lambda0 * crit_lm(cbs, cb_y.view(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm(ner_model.parameters(), args.clip_grad)\n",
    "            optimizer.step()\n",
    "\n",
    "    epoch_loss /= tot_length\n",
    "\n",
    "    # update lr\n",
    "    utils.adjust_learning_rate(optimizer, args.lr / (1 + (args.start_epoch + 1) * args.lr_decay))\n",
    "\n",
    "    # eval & save check_point\n",
    "    if 'f' in args.eva_matrix:\n",
    "        dev_f1, dev_pre, dev_rec, dev_acc = evaluator.calc_score(ner_model, dev_dataset_loader[file_no], file_no)\n",
    "\n",
    "        if dev_f1 > best_f1[file_no]:\n",
    "            patience_count = 0\n",
    "            best_f1[file_no] = dev_f1\n",
    "            best_pre[file_no] = dev_pre\n",
    "            best_rec[file_no] = dev_rec\n",
    "\n",
    "            test_f1, test_pre, test_rec, test_acc = evaluator.calc_score(ner_model, test_dataset_loader[file_no], file_no)\n",
    "\n",
    "            track_list.append(\n",
    "                {'loss': epoch_loss, 'dev_f1': dev_f1, 'dev_acc': dev_acc, 'test_f1': test_f1,\n",
    "                 'test_acc': test_acc})\n",
    "\n",
    "            print(\n",
    "                '(loss: %.4f, epoch: %d, dataset: %d, dev F1 = %.4f, dev pre = %.4f, dev rec = %.4f, F1 on test = %.4f, pre on test= %.4f, rec on test= %.4f), saving...' %\n",
    "                (epoch_loss,\n",
    "                 args.start_epoch,\n",
    "                 file_no,\n",
    "                 dev_f1,\n",
    "                 dev_pre,\n",
    "                 dev_rec,\n",
    "                 test_f1,\n",
    "                 test_pre,\n",
    "                 test_rec))\n",
    "\n",
    "            if args.output_annotation: #NEW\n",
    "                print('annotating')\n",
    "                with open('output'+str(file_no)+'.txt', 'w') as fout:\n",
    "                    predictor.output_batch(ner_model, test_word[file_no], fout, file_no)\n",
    "\n",
    "            try:\n",
    "                utils.save_checkpoint({\n",
    "                    'epoch': args.start_epoch,\n",
    "                    'state_dict': ner_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'f_map': f_map,\n",
    "                    'l_map': l_map,\n",
    "                    'c_map': char_map,\n",
    "                    'in_doc_words': in_doc_words\n",
    "                }, {'track_list': track_list\n",
    "                    }, args.checkpoint + 'cwlm_lstm_crf')\n",
    "            except Exception as inst:\n",
    "                print(inst)\n",
    "\n",
    "        else:\n",
    "            patience_count += 1\n",
    "            print('(loss: %.4f, epoch: %d, dataset: %d, dev F1 = %.4f, dev pre = %.4f, dev rec = %.4f)' %\n",
    "                  (epoch_loss,\n",
    "                   args.start_epoch,\n",
    "                   file_no,\n",
    "                   dev_f1,\n",
    "                   dev_pre,\n",
    "                   dev_rec))\n",
    "            track_list.append({'loss': epoch_loss, 'dev_f1': dev_f1, 'dev_acc': dev_acc})\n",
    "\n",
    "    print('epoch: ' + str(args.start_epoch) + '\\t in ' + str(args.epoch) + ' take: ' + str(\n",
    "        time.time() - start_time) + ' s')\n",
    "\n",
    "    if patience_count >= args.patience and args.start_epoch >= args.least_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0f9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
